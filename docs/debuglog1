Segfault debug for Memcached with Async page faults
===================================================

[81334.690697] show_signal: 6 callbacks suppressed
[81334.690697] traps: memcached[13981] general protection fault ip:4210de sp:7f605a49ec90 error:0 in memcached[403000+3a000]

[251114.312324] iokerneld[156108]: segfault at 10 ip 000000000082a0f0 sp 00007fff7af625f0 error 4 in iokerneld[409000+b66000]
[251114.312338] Code: 41 8b 02 4c 8b 44 24 18 41 89 42 08 64 41 8b 29 83 fd 7f 40 0f 97 c6 e9 af fd ff ff 66 0f 1f 44 00 00 4c 89 63 18 48 8d 70 10 <0f> b7 50 10 66 83 fa 01 0f 84 7a fe ff ff 66 f0 ff 0e 0f 85 3e fe

q
oops handler 
run with gdb - 
turn off ASLR and print some random ips - trail and error 


sudo RDMA_RACK_CNTRL_IP=192.168.0.40 RDMA_RACK_CNTRL_PORT=9202 MEMORY_LIMIT=1600000000 EVICTION_THRESHOLD=0.99 EVICTION_DONE_THRESHOLD=0.99 EVICTION_BATCH_SIZE=1 numactl -N 1 -m 1 gdbserver :1234 /home/ayelam/rmem-scheduler/memcached/memcached memcached.config -u ayelam -t 4 -U 5033 -p 5033 -c 32768 -m 32000 -b 32768 -P memcached_pid -r -o hashpower=28,no_hashexpand,no_lru_crawler,no_lru_maintainer,idle_timeout=0 2>&1 | ts %s  > memcached.out


sudo RDMA_RACK_CNTRL_IP=192.168.0.40 RDMA_RACK_CNTRL_PORT=9202 MEMORY_LIMIT=1600000000 EVICTION_THRESHOLD=0.99 EVICTION_DONE_THRESHOLD=0.99 EVICTION_BATCH_SIZE=1 numactl -N 1 -m 1 gdbserver :1234 /home/ayelam/rmem-scheduler/memcached/memcached memcached.config -u ayelam -t 4 -U 5035 -p 5035 -c 32768 -m 32000 -b 32768 -P memcached_pid -r -o hashpower=28,no_hashexpand,no_lru_crawler,no_lru_maintainer,idle_timeout=0


sudo RDMA_RACK_CNTRL_IP=192.168.0.40 RDMA_RACK_CNTRL_PORT=9202 MEMORY_LIMIT=1600000000 EVICTION_THRESHOLD=0.99 EVICTION_DONE_THRESHOLD=0.99 EVICTION_BATCH_SIZE=1  numactl -N 1 -m 1 gdbserver :1234 /home/ayelam/rmem-scheduler/memcached/memcached memcached.config -u ayelam -t 4 -U 5231 -p 5231 -c 32768 -m 32000 -b 32768 -P memcached_pid -r -o hashpower=28,no_hashexpand,no_lru_crawler,no_lru_maintainer,idle_timeout=0 2>&1 | ts %s 



Reading /lib/x86_64-linux-gnu/.debug/libnss_nis-2.31.so from remote target...
Reading /usr/lib/debug//lib/x86_64-linux-gnu/libnss_nis-2.31.so from remote target...
Reading /usr/lib/debug//lib/x86_64-linux-gnu/libnss_nis-2.31.so from remote target...
Reading /lib/x86_64-linux-gnu/libnsl-2.31.so from remote target...
Reading /lib/x86_64-linux-gnu/.debug/libnsl-2.31.so from remote target...
Reading /usr/lib/debug//lib/x86_64-linux-gnu/libnsl-2.31.so from remote target...
Reading /usr/lib/debug//lib/x86_64-linux-gnu/libnsl-2.31.so from remote target...
Reading /lib/x86_64-linux-gnu/libnss_files-2.31.so from remote target...
Reading /lib/x86_64-linux-gnu/.debug/libnss_files-2.31.so from remote target...
Reading /usr/lib/debug//lib/x86_64-linux-gnu/libnss_files-2.31.so from remote target...
Reading /usr/lib/debug//lib/x86_64-linux-gnu/libnss_files-2.31.so from remote target...


[New Thread 165805.168653]
[New Thread 165805.168608]
[New Thread 165805.168609]
[New Thread 165805.168610]
[New Thread 165805.168611]
[New Thread 165805.168651]
[New Thread 165805.168652]
--Type <RET> for more, q to quit, c to continue without paging--

Thread 2 "memcached" received signal SIGSEGV, Segmentation fault.
[Switching to Thread 165805.168653]
0x00000000004210de in mbuf_free (m=0x7ff75446dea8) at ./inc/net/mbuf.h:265
265             m->release(m);
(gdb) 
(gdb) backtrace
#0  0x00000000004210de in mbuf_free (m=0x7ff75446dea8) at ./inc/net/mbuf.h:265
#1  softirq_fn (arg=arg@entry=0x7ff7502d7cb0) at runtime/softirq.c:29
#2  0x0000000000421850 in softirq_run (budget=budget@entry=16) at runtime/softirq.c:176
#3  0x000000000041fcc8 in thread_yield () at runtime/sched.c:465
#4  0x000000000041dacb in __possible_fault_on (flags=1, address=<optimized out>) at runtime/pgfault.c:137
#5  possible_read_fault_on (address=address@entry=0x7ff813721669) at runtime/pgfault.c:176
#6  0x0000000000416f5c in assoc_find (key=key@entry=0x6bd4b68 "9266215", 'K' <repeats 21 times>, nkey=nkey@entry=20, hv=hv@entry=4147491564)
    at assoc.c:89
#7  0x0000000000415f4c in do_item_get (key=key@entry=0x6bd4b68 "9266215", 'K' <repeats 21 times>, nkey=nkey@entry=20, hv=hv@entry=4147491564, 
    c=c@entry=0x6bd49c0, do_update=do_update@entry=false) at items.c:936
#8  0x000000000041758f in item_get (key=key@entry=0x6bd4b68 "9266215", 'K' <repeats 21 times>, nkey=nkey@entry=20, c=c@entry=0x6bd49c0, 
    do_update=do_update@entry=false) at thread.c:598
#9  0x000000000040a84d in process_bin_get_or_touch (c=0x6bd49c0) at memcached.c:1638
#10 complete_nread_binary (c=c@entry=0x6bd49c0) at memcached.c:2668
#11 0x000000000040d22c in complete_nread (c=0x6bd49c0) at memcached.c:2719
#12 drive_machine (arg=0x6bd49c0) at memcached.c:5522
#13 0x000000000041e9f0 in ?? () at runtime/sched.c:122

fixed iokernel segfault bug, had to do with assert statement
tried fixing double frees and got this error 

1646417549 profiler,7046,0,14364,3247,0,0,0,574,31897,8444,0,1566,6936,0,9541,0,12034,6256,0,15136
1646417550 *** stack smashing detected ***: terminated
try again with gdb for more info?
Still seeing the above segfault though 

(gdb) backtrace
#0  0x000000000041fc54 in mbuf_free (m=0x7ff7545b2d28) at ./inc/net/mbuf.h:265
#1  softirq_fn (arg=arg@entry=0x7ff751ad7cb0) at runtime/softirq.c:31
#2  0x00000000004202d6 in softirq_run (budget=budget@entry=16) at runtime/softirq.c:181
#3  0x000000000041ebd8 in thread_yield () at runtime/sched.c:465
#4  0x000000000041d56d in __possible_fault_on (flags=1, address=<optimized out>) at runtime/pgfault.c:137
#5  possible_read_fault_on (address=address@entry=0x7ff81121d229) at runtime/pgfault.c:176
#6  0x0000000000416ebc in assoc_find (key=key@entry=0x6c23be8 "0954242", 'K' <repeats 21 times>, 
    nkey=nkey@entry=20, hv=hv@entry=196096119) at assoc.c:89
#7  0x0000000000415edc in do_item_get (key=key@entry=0x6c23be8 "0954242", 'K' <repeats 21 times>, 
    nkey=nkey@entry=20, hv=hv@entry=196096119, c=c@entry=0x6c23a40, do_update=do_update@entry=false)
    at items.c:936
#8  0x00000000004174af in item_get (key=key@entry=0x6c23be8 "0954242", 'K' <repeats 21 times>, 
    nkey=nkey@entry=20, c=c@entry=0x6c23a40, do_update=do_update@entry=false) at thread.c:598
#9  0x000000000040a84d in process_bin_get_or_touch (c=0x6c23a40) at memcached.c:1638
#10 complete_nread_binary (c=c@entry=0x6c23a40) at memcached.c:2668
#11 0x000000000040d22c in complete_nread (c=0x6c23a40) at memcached.c:2719
#12 drive_machine (arg=0x6c23a40) at memcached.c:5522
#13 0x000000000041e010 in ?? () at runtime/sched.c:739

full trace:
1647179146 No locals.
1647179146 #1  softirq_fn (arg=0x7ff7520dafe0) at runtime/softirq.c:31
1647179146         w = 0x7ff7520dafe0
1647179146         i = <optimized out>
1647179146 #2  0x000000000041e010 in ?? () at runtime/sched.c:739
1647179147         thread_tcache = 0x5e4c60
1647179147         thread_slab = {name = 0x441ecf "runtime_threads", size = 192, link = {next = 0x464f90 <smalloc_slabs+16>, prev = 0x54ad10 <node_slab+16>}, 
1647179147           nodes = {0x1000000080c0, 0x1010000080c0, 0x0, 0x0}}
1647179147         runtime_stack_base = <optimized out>
1647179147         last_watchdog_tsc = 0
1647179147         runtime_stack = 0x7ff75211bff8
1647179147         last_tsc = 14354775559919106
1647179147         disable_watchdog = true
1647179147         __self = 0x100000009c00

TODO print all mbufs at creation and check if the failed one shows up?

REgion between 0x7ff774600000 and 0x7ff77464c000
m = 0x7ff7544066e8

1647308102 #0  0x000000000041fc4e in mbuf_free (m=0x7ff7544066e8) at ./inc/net/mbuf.h:265
1647308102 No locals.
1647308102 #1  softirq_fn (arg=0x7ff750e17fe0) at runtime/softirq.c:31
1647308102         w = 0x7ff750e17fe0
1647308102         i = <optimized out>
1647308102 #2  0x000000000041e010 in ?? () at runtime/sched.c:739
1647308102         thread_tcache = 0x5e4c60
1647308102         thread_slab = {name = 0x441ecf "runtime_threads", size = 192, link = {
1647308102             next = 0x464f90 <smalloc_slabs+16>, prev = 0x54ad10 <node_slab+16>}, 
1647308102           nodes = {0x1000000080c0, 0x1010000080c0, 0x0, 0x0}}
1647308102         runtime_stack_base = <optimized out>
1647308102         last_watchdog_tsc = 0
1647308102         runtime_stack = 0x7ff75211bff8
1647308102         last_tsc = 14636964229855680
1647308102         disable_watchdog = true
1647308102         __self = 0x10000001dd80

p ((struct softirq_work *)(0x7ff750e17fe0))->compl_cnt
p ((struct softirq_work *)(0x7ff750e17fe0))->recv_cnt
p ((struct softirq_work *)(0x7ff750e17fe0))->join_cnt
p ((struct softirq_work *)(0x7ff750e17fe0))->fault_cnt

budget_left was going negative, fixed it.
but how exactly was that leading to a seg-fault? 

now it crashes silently i.e., shenango threads crank to a halt.
reserving budget for timers didn't help.
no asserts triggered
what if scheduler is stuck in threads that never progress?
    try enabling watchdog but thread_yield() looks at softirqs all the time...
    disabling steals and parks didn't help

doesn;t look like the threads are stuck at transmit waiting for mbufs
                do {
                    static uint64_t __last_us = 0;
                    static uint64_t __suppressed = 0;
                    uint64_t __cur_us = microtime();
                    if (__cur_us - __last_us >= ONE_SECOND) {
                        if (__suppressed) {	
                            printf("could not xmit: suppressed %ld times\n", __suppressed);
                            __suppressed = 0;
                        }			
                        printf("could not xmit\n");
                        __last_us = __cur_us;	
                    } else				
                        __suppressed++;	
                } while(0);	

    
gdb collect traces; if not , try intel ptrace

1647452760 
1647452760 Thread 3 "kona_eviction" received signal SIG33, Real-time event 33.
1647452760 
1647452760 Thread 5 "kona_poller" received signal SIG33, Real-time event 33.
1647452760 
1647452760 Thread 8 "memcached" received signal SIG33, Real-time event 33.
1647452760 
1647452760 Thread 7 "memcached" received signal SIG33, Real-time event 33.
1647452760 
1647452760 Thread 1 "memcached" received signal SIG33, Real-time event 33.

Looks like the block is happening in Kona.

try no writes to avoid write protection faults to Kona?

introduced throtting in kona where requests are not accepted if response queue is 
filling fast. that solved it!

Ok i called it too soon. It's still there and in a more elusive form.
Mar 18 20:28:06 sc2-hs2-b1630 gnome-shell[3054]: No cursor theme available, please install a cursor theme
Mar 18 20:28:30 sc2-hs2-b1630 systemd[1]: fprintd.service: Succeeded.
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449513] INFO: task memcached:13482 blocked for more than 120 seconds.
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449522]       Tainted: G           OE     5.11.0-rc1+ #8
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449524] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449527] task:memcached       state:D stack:    0 pid:13482 ppid: 13321 flags:0x00000000
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449532] Call Trace:
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449537]  __schedule+0x44c/0x8a0
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449545]  schedule+0x4f/0xc0
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449549]  rwsem_down_read_slowpath+0x184/0x3c0
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449555]  down_read+0x43/0xa0
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449559]  do_user_addr_fault+0x3a7/0x540
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449564]  exc_page_fault+0x6c/0x150
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449570]  ? asm_exc_page_fault+0x8/0x30
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449572]  asm_exc_page_fault+0x1e/0x30
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449575] RIP: 0033:0x7ffff7fcde07
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449578] RSP: 002b:00007ff6b021ed00 EFLAGS: 00010246
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449582] RAX: 0000000000000000 RBX: 0000000000000014 RCX: 000000000000001c
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449584] RDX: 00000000077f6680 RSI: 000000000000001c RDI: 00007ff7fc302000
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449586] RBP: 00007ff6b021ed00 R08: 0000000000000000 R09: 0000000005f58968
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449588] R10: 0000000000000000 R11: 0000000000000000 R12: 00007ff7fc302980
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449590] R13: 00000000a77f6680 R14: 00000000a77f6680 R15: 0000000005f587c0
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449595] INFO: task kona_eviction:13493 blocked for more than 120 seconds.
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449598]       Tainted: G           OE     5.11.0-rc1+ #8
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449600] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449601] task:kona_eviction   state:D stack:    0 pid:13493 ppid: 13321 flags:0x00000000
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449605] Call Trace:
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449607]  __schedule+0x44c/0x8a0
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449611]  schedule+0x4f/0xc0
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449615]  rwsem_down_read_slowpath+0x184/0x3c0
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449619]  down_read+0x43/0xa0
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449623]  do_madvise+0x86e/0xda0
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449628]  ? recalibrate_cpu_khz+0x10/0x10
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449635]  ? lapic_next_deadline+0x26/0x30
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449640]  ? hrtimer_interrupt+0x13b/0x220
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449647]  __x64_sys_madvise+0x2a/0x30
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449650]  ? syscall_exit_to_user_mode+0x27/0x50
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449655]  ? __x64_sys_madvise+0x2a/0x30
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449657]  do_syscall_64+0x38/0x90
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449661]  entry_SYSCALL_64_after_hwframe+0x44/0xa9
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449663] RIP: 0033:0x7ffff7e63bdb
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449665] RSP: 002b:00007ffff6850de8 EFLAGS: 00000206 ORIG_RAX: 000000000000001c
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449669] RAX: ffffffffffffffda RBX: 0000000000000001 RCX: 00007ffff7e63bdb
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449671] RDX: 0000000000000004 RSI: 0000000000001000 RDI: 00007ff817d0e000
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449672] RBP: 000000000036c049 R08: 00007ffff5842188 R09: 00007ffff6850cc0
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449674] R10: 0000000000000001 R11: 0000000000000206 R12: 00000000af418cea
Mar 18 20:28:36 sc2-hs2-b1630 kernel: [ 1089.449676] R13: 00007ff817d0e000 R14: 000000000054d540 R15: 00007ffff7fa2000

can't work with gdb. the terminals hang the moment crash occurs. 
try logging in from the ILO?
Evne with ILO, gdb loses control of the process after the error - this bug just got worse.
Even external kill signals won't deliver.
OK NOTHING WORKS. can't get a coredump.

try strace?
TODO: try allowing new rx only if pf_pending is small
try lower load again

1647823638 counters,463744,469139,3425,0,0,350889,0,0,
541544,541545,541544,541545,469871,0,0,0,469871,469871,541545,68,463173,469871,469871,469871,0,0,0,0,1920991232,0,0,1600000000,0,
450435,450073,0,450407,362,362,449406,449406,0,0

TODO
steadily increase max in flight..
also, try lower load
TODO: try allowing new rx only if pf_pending is small

saw shenango stuck at this:
50321,50321,0,50321,0,0,50315,50315,0,0
even when all the app faults are serviced! 
so kona is not servicing normal faults still? 
TODO make sure kona is handling all the normal faults as well.
TODO instrument normal faults...
if it does not, how does the SYNC case work? Try it once.

EDGE CASE: what if there are concurrent faults one from app fault and one from kernel?
when an app fault is in progress, if there is a kernel fault on the same page,
then we let the kernel fault dissipate - but we never respond to it to release the 
thread - or does the uffd_copy do that? Print that case to see if it correlates with the bug!
what if there is a race?
TODO how about collecting a detailed trace of the lifestages of the kernel and app faults?

EDGE CASE: Could the madvise notif race be manifesting as this bug now?

Kona doesn't seem to be the problematic component anymore. every fault, kernel and the app ones, return 
and there doesn;t seem to be any concurrent faults that might cause issues like leaving kernel threads hanging.
are coming back. Focus on shenango now?

thread states:
    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND                                                                                                                                             
  67288 ayelam    20   0   37.1g   3.6g   4268 t   0.0   1.9   0:14.63 memcached                                                                                                                                           
  67289 root      20   0   37.1g   3.6g   4268 t   0.0   1.9   1:41.42 kona_fault                                                                                                                                          
  67290 root      20   0   37.1g   3.6g   4268 D   0.0   1.9   1:37.23 kona_eviction                                                                                                                                       
  67291 root      20   0   37.1g   3.6g   4268 t   0.0   1.9   0:00.00 kona_accnt                                                                                                                                          
  67292 root      20   0   37.1g   3.6g   4268 t   0.0   1.9   1:41.40 kona_poller                                                                                                                                         
  67293 ayelam    20   0   37.1g   3.6g   4268 D   0.0   1.9   0:14.19 memcached                                                                                                                                           
  67294 root      20   0   37.1g   3.6g   4268 D   0.0   1.9   0:14.02 memcached                                                                                                                                           
  67295 root      20   0   37.1g   3.6g   4268 D   0.0   1.9   0:13.94 memcached  

QUESTION why is eviction getting blocked? is it getting blocked right away?

[77335.544668] INFO: task kona_eviction:55770 blocked for more than 362 seconds.
[77335.544670]       Tainted: G           OE     5.11.0-rc1+ #8
[77335.544673] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[77335.544674] task:kona_eviction   state:D stack:    0 pid:55770 ppid: 55766 flags:0x00000000
[77335.544677] Call Trace:
[77335.544679]  __schedule+0x44c/0x8a0
[77335.544683]  ? free_pages_and_swap_cache+0xb9/0xd0
[77335.544689]  schedule+0x4f/0xc0
[77335.544692]  rwsem_down_read_slowpath+0x184/0x3c0
[77335.544696]  down_read+0x43/0xa0
[77335.544700]  mwriteprotect_range+0x5b/0x150
[77335.544705]  userfaultfd_ioctl+0x126/0x1090
[77335.544711]  ? _copy_from_user+0x3f/0x80
[77335.544717]  __x64_sys_ioctl+0x91/0xc0
[77335.544720]  ? __x64_sys_ioctl+0x91/0xc0
[77335.544722]  do_syscall_64+0x38/0x90
[77335.544726]  entry_SYSCALL_64_after_hwframe+0x44/0xa9

Smells like a deadlock in the kernel. 
TODO (done): register madvise notif and re-run. If that's the issue, then why block in write-protect? -- this is of no help.
TODO: go look at uffd kernel code at the these places.
TODO (done): run with write faults annotated   -- same issue. annotating at a known location removed write faults 
        but there are still a bunch of pure WRITE fault locations that take the kernel path.
Think about the deadlock!

Error with register madvise notif:
1647969089 profiler,11539,0,0,0,8912,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1647969089 [32m++[klib_uffd.c] uffd_copy err EEXIST but wake_on_exist=false: 7fe94bb49000
1647969089 [0mmemcached: klib_uffd.c:194: uffd_copy: Assertion `(0)' failed.

TODO Intel PT:
WARNING: perf not found for kernel 5.11.0: 
Ask Radhika about building perf tools
Ask for help with PT as well

TODO: Is the do-not-evict invariant getting violated? Increase the size of DNE? -- did not seem to have any effect
TODO: Notes/suggestions from Marcos
1. Try with a simpler setting that still reproes the bug
trying with one core:
run-03-24-18-24 SYNC case
run-03-24-18-36 ASYNC case - sees a drop in throughput. WHY? -- actually xput is much better but the 
charts don't show it because the printing is messed up in the ASYNC case. Why?
Trying 2 cores now: repro'ed.
there are considerably more mmaps in the ASYNC case.. which is understandable?


Limiting the number of outstanding threads to READY_Q_SIZE sidestepped the bug.

But there seem to be some variables controlling this, so these may be valuable in future as well?
TODO (done) Need to make the throttling logic work stealing-safe.
TODO (done) How does the number of active threads affect the final performance 
TODO (done) How does max kona faults in flight affect the performance 
TODO Need to only block RX in case of throttling, not TX completions and other softirqs. 
TODO How did this bug happen? 
TODO Need to do a post-mortem and what I learned 

Maintianing more active threads doesn't seem to have any effect on performance: run-03-25-16*
Increasing kona concurrent fault handling limit increases performance but only modestly: run-03-25-21

Got full set of runs.
Overall Baseline: Kernel faults
1 core: 03-25-23 
2 cores: 03-26-00
3 cores: 03-26-01-[0123]
4 cores: run-03-26-\(01-[45]\|02-[012]\)
5 cores: run-03-26-\(02-[345]\|03-[01]\)
6 cores: 03-29-1[67]
Scheduler faults Baseline - Synchronous case 
1 core: 03-26-09-[0-5]
2 cores: 03-26-10-[0-3]
3 cores: run-03-26-\(10-[45]\|11-[012]\)
4 cores: run-03-26-\(11-[345]\|12-[01]\)
5 cores: 03-26-12-[2-5]
6 cores: 03-29-22
Scheduler faults Baseline - Asynchronous case 
1 core: run-03-26-\(16-[345]\|17-[01]\)
2 cores: 03-26-17-[2345]
3 cores: 03-26-18
4 cores: 03-27-14-[0123]
5 cores: run-03-27-\(14-[45]\|15\)
6 cores: 03-30-[01]


** per fault kind **
bash scripts/plotg.sh -s1="03-25-23" -l1="1" -s2="03-26-00" -l2="2"                 \
    -s3="03-26-01-[0123]" -l3="3" -cs4="run-03-26-\(01-[45]\|02-[012]\)" -l4="4"    \
    -cs5="run-03-26-\(02-[345]\|03-[01]\)" -l5="5" -s6="03-29-1[67]" -l6="6" -lt="App CPU (KERNEL)"

bash scripts/plotg.sh -s1="03-26-09-[0-5]" -l1="1" -s2="03-26-10-[0-3]" -l2="2"     \
    -cs3="run-03-26-\(10-[45]\|11-[012]\)" -l3="3" -s5="03-26-12-[2-5]" -l5="5"     \
    -cs4="run-03-26-\(11-[345]\|12-[01]\)" -l4="4" -s6="03-29-22" -l6="6" -lt="App CPU (SYNC)"

bash scripts/plotg.sh -cs1="run-03-26-\(16-[345]\|17-[01]\)" -l1="1"                \
    -s2="03-26-17-[2345]" -l2="2" -s3="03-26-18" -l3="3" -s4="03-27-14-[0123]"       \
    -l4="4" -cs5="run-03-27-\(14-[45]\|15\)" -l5="5" -s6="03-30-[01]" -l6="6" -lt="App CPU (ASYNC)"

** per core **
bash scripts/plotg.sh -lt="Fault Type (1 core)"                         \
    -s1="03-25-23"                              -l1="Kernel"            \
    -s2="03-26-09-[0-5]"                        -l2="Scheduler(SYNC)"   \
    -cs3="run-03-26-\(16-[345]\|17-[01]\)"      -l3="Scheduler(ASYNC)"

bash scripts/plotg.sh -lt="Fault Type (2 cores)"                        \
    -s1="03-26-00"                              -l1="Kernel"            \
    -s2="03-26-10-[0-3]"                       -l2="Scheduler(SYNC)"    \
    -s3="03-26-17-[2345]"                       -l3="Scheduler(ASYNC)"

bash scripts/plotg.sh -lt="Fault Type (3 cores)"                        \
    -s1="03-26-01-[0123]"                       -l1="Kernel"            \
    -cs2="run-03-26-\(10-[45]\|11-[012]\)"      -l2="Scheduler(SYNC)"   \
    -s3="03-28-11"                              -l3="Scheduler(ASYNC)"

bash scripts/plotg.sh -lt="Fault Type (4 cores)"                        \
    -cs1="run-03-26-\(01-[45]\|02-[012]\)"      -l1="Kernel"            \
    -cs2="run-03-26-\(11-[345]\|12-[01]\)"      -l2="Scheduler(SYNC)"   \
    -s3="03-27-14-[0123]"                       -l3="Scheduler(ASYNC)"



*** specific format to compare with the similation results ***
krate=150000
outdir=.
bash scripts/plotg.sh -s1="03-25-23" -l1="1" -s2="03-26-00" -l2="2"                 \
    -s3="03-26-01-[0123]" -l3="3" -cs4="run-03-26-\(01-[45]\|02-[012]\)" -l4="4"    \
    -cs5="run-03-26-\(02-[345]\|03-[01]\)" -l5="5" -lt="noup"
bash scripts/plotg.sh -cs1="run-03-26-\(16-[345]\|17-[01]\)" -l1="1"                \
    -s2="03-26-17-[2345]" -l2="2" -s3="03-26-18" -l3="3" -s4="03-27-14-[0123]"      \
    -l4="4" -cs5="run-03-27-\(14-[45]\|15\)" -l5="5" -lt="up"
python3 scripts/plot.py    \
    -xc konamem -xl "Local Hit Ratio" -yc achieved -yl "MOPS" --ymul 1e-6  \
    -d ${outdir}/xput_up_1_${krate}    -l "1"   -ls solid   -cmi 0      \
    -d ${outdir}/xput_noup_1_${krate}    -l ""    -ls dashed  -cmi 1    \
    -d ${outdir}/xput_up_2_${krate}    -l "2"   -ls solid   -cmi 0      \
    -d ${outdir}/xput_noup_2_${krate}    -l ""    -ls dashed  -cmi 1    \
    -d ${outdir}/xput_up_3_${krate}    -l "3"   -ls solid   -cmi 0      \
    -d ${outdir}/xput_noup_3_${krate}    -l ""    -ls dashed  -cmi 1    \
    -d ${outdir}/xput_up_4_${krate}    -l "4"   -ls solid   -cmi 0      \
    -d ${outdir}/xput_noup_4_${krate}    -l ""    -ls dashed  -cmi 1    \
    -d ${outdir}/xput_up_5_${krate}    -l "5"   -ls solid   -cmi 0      \
    -d ${outdir}/xput_noup_5_${krate}    -l ""    -ls dashed  -cmi 1    \
    --size 4.5 3 -fs 11 -of png -o plots/sim_vs_real_xput.png -s

** ASYNC run with warmup to avoid eviction step: run-03-28-11
